\section{Reconstructing Full Scalar Fields} % (fold)
\label{sec:reconstruction}
%
If desired, the scalar fields on the original grid can be reconstructed from the
sparse representation by interpolation.
%
We sample the fitted models in regular intervals between the respective $x_l$
and $x_r$ of each line and variable.
%
We then apply standard interpolation methods to this set of points to
reconstruct the data on the original grid.
%

% Remember that when extracting the profiles, we deliberately ignored all
% data farther away from the flame surface than half the length of the sampled
% line. When reconstructing the data, we therefore only consider the region inside
% the convex hull of the point cloud resulting from sampling the fitted model
% functions. The remaining volume is assumed to contain only constant values.
%
% The interpolation scheme employed here has to be able to handle scattered data,
% must be efficient, and has to produce results as close as possible to the
% original data. There are generally three approaches for interpolating scattered
% data: Using a global method and generating new values from a combination of all
% data points, using only the local $k$ nearest data points to do the same, or
% generating a mesh with the data points as vertices and interpolating values
% inside the resulting cells using some mesh interpolation scheme. Global methods
% are not suitable here, as they are very inefficient. For each value that is
% interpolated, an expression involving all data points has to be evaluated. 

%
We compared two local interpolation methods for scattered data.
%
The first method is a \ac{kANN}~\cite{Arya1998} interpolation scheme that weighs
the values of the $k$ approximate nearest neighbors using Shepard's inverse
distance weights.
%
% For large $k$ the result converges to the
% global Shepard interpolation but computation time increases. For small $k$ the
% computation is faster but the result has more discontinuities where the sets of
% nearest neighbors change.
%
% \begin{equation}
% \label{eqn:shepardweights}
%   f(\vx) = \sum_{j=1}^k \frac{w_{i_j}(\vx)f_{i_j}}{\sum_{l=1}^k w_{i_l}(\vx)}, ~
%   \text{with} ~ w_{i_j}(\vx) = \frac{1}{||\vx-\vx_{i_j}||^p}
% \end{equation}
% and $i_j, ~j=1,\dotsc, k$ are the indices of the $k$ approximate nearest
% neighbors of $\vx$.
% Since the $k$ approximate nearest neighbors can be found efficiently using a kd-
% tree and the Shepard weights are easily computable, this method is very fast.
%
The second interpolation method first generates a tetrahedral mesh from the data
points using a Delaunay triangulation.
%
The values inside the mesh cells are then linearly interpolated.
%
This always produces a continuous solution if there are no degenerate mesh
cells.
%

%
Interpolation methods providing higher smoothness exist.
%
However, these are more computationally expensive and it is not guaranteed that
they produce results closer to the original data than the simpler methods.
%

% Evaluation of compression scheme to assess compression errors
% Used three different data sets
% describe data sets
%
We evaluated the accuracy of the sparse representation on single time steps of
three data sets.
%
Each time step of data sets \textsc{Syngas~I} and \textsc{Syngas~II} has
$\num{200}^3$ voxels and \num{13} variables each.
%
\textsc{Syngas~III} has $\num{100}^3$ voxels and \num{3} variables.
%
All data sets are from \ac{DNS} computations of turbulent premixed spherical
syngas flames.
%
\textsc{Syngas~I} contains a flame with strong wrinkles.
%
\textsc{Syngas~II} has a flame with smaller wrinkles.
%
\textsc{Syngas~III} contains a flame that has been torn into smaller parts by
turbulence.
%
% Data sets \textsc{Syngas~I} and \textsc{Syngas~II} are from DNS
% computations of turbulent premixed spherical syngas flames. Each time-step has
% $200^3$ voxels and $13$ variables each. Data set \textsc{Hydrogen} is from a
% turbulent premixed spherical hydrogen flame and has a resolution of $400^3$
% voxels and $11$ variables. This is a size our application partner routinely uses
% for simulations. \textsc{Syngas~I} contains a flame with strong wrinkles.
% \textsc{Syngas~II} has a flame with smaller wrinkles. \textsc{Hydrogen} is much
% bigger but with a less wrinkled surface.
%
%, which yields a very similar surface, is used instead.
%
% 1. Errors made by fitting the models to the profile lines (\ac{RMS} error)
% Normalized by data value range to make comparable
% Shows raw error of models
% Results show low percentual errors for all variables

%
First, we investigate the error from approximating the original data by our
models.
%
We computed the average \ac{RMS} error between the original data of the profiles
and the fitted models.
%
The data values in the range $x_{li}$ and $x_{ri}$ on each profile line are
considered.
%
%(Figure~\ref{fig:models}), because values outside of this interval were not considered when fitting the models.
%
We used normalized \ac{RMS} errors $E^V_{\textnormal{fit}}$ in order to make the
variables $V$ comparable:
%
%To make the values comparable, the resulting average
%\ac{RMS} error for each variable was normalized by the range of its values.
%The fitting error for a variable $V$ is then:
%
\begin{equation}
    E^V_{\textnormal{fit}}
        =   \frac{1}{n \cdot (\max(V)-\min(V))}
            \Sum{
                \sqrt{
                    \frac{1}{x_{ri}-x_{li}}
                    \Sum{
                        \left(P_{ij}^V-u_{ij}^V\right)^2}
                        {\{j | x_{li} \leq x_{ji} \leq x_{ri}\}}}}
            {i, 1, n}
    ~\text{.}
\end{equation}
%
Here, $u_{ij}^V$ is the value of the fitted model corresponding to the original
profile value $P_{ij}^V$, while $\{j | x_{li} \leq x_{ji} \leq x_{ri}\}$ are the
indices of all points on the profile between $x_{li}$ and $x_{ri}$. We computed
this error for all possible profile lines in all data sets.
% 
% For smoothing the profiles before finding the feature points, we used a
% Gaussian kernel with $\sigma = 1 \text{ voxel}$.
%
As \cref{fig:fitting_rms_error} shows, the errors are quite low, ranging from
\SI{0.1}{\percent} to \SI{6.3}{\percent} of the respective variable's range.

For investigating the overall error after reconstruction, We computed the
reduction ratio $c$ for different $q$ and compared it to the
deviation from the original data after reconstruction. The reduction ratio is
defined as the storage space needed for the original data divided by the space
needed by the sparse representation.
%
% The most important
% measure for the quality of a lossy compression scheme is the relation of the
% reduction ratio to the compression error. In our approach, the compression
% ratio is determined by the number of profile lines seeded on the flame surface,
% and by the number and type of variables contained in the data set. The locations
% and directions of the profile lines $\vp_i$ and $\vr_i$ only have to
% be stored once for all variables. Thus, with more variables in the data set, the
% space needed for this information becomes less significant. Also, the more
% reactants and products are contained in the data set, compared to the number of
% intermediate species, the better the reduction ratio, since the sigmoid
% model used for representing those species needs seven parameters, while the
% Gaussian model needs eight. 
% Most significant for the reduction ratio, however, is the number of seeded
% profile lines. This is steered by the value of $q$, but mainly
% depends on the data itself. Higher data resolution means that more lines are
% seeded. More importantly, the size and structure of the flame itself is
% responsible for the reduction ratio. A small flame naturally results in fewer
% profile lines seeded on its surface. If the flame is very smooth and does not
% exhibit a lot of wrinkles, fewer lines are seeded also, due to the low surface
% curvature. Big, strongly wrinkled flames will need much more profile lines to be
% represented accurately, resulting in a lower reduction ratio.
% \todo{explain this earlier. Especially emphasize data dependence of
% seeding and therefore reduction ratio.}
%
% \begin{equation}
%   c
%       = \frac{B_\text{original}}{B_\text{sparse}}
% \end{equation}
%
% Where $B_\text{original}$ and $B_\text{sparse}$ are the storage space needed to
% store the original and transformed data. 
%
% The sparse representation consists of the vertices and faces of the flame
% surface mesh, the indices of the vertices selected for seeding profiles, the
% directions $\vr_i$ of the profile lines, and the model parameters for each
% simulation variable and profile.
%
%For measuring the reconstruction error, we computed the mean square errors for
%ach variable in the data set after compressing and decompressing them.
%
%The error was normalized by the range of values of the variable, to make the errors
%for different variables comparable.
%
We used a normalized error metric to compute the reconstruction quality:
%
\begin{equation}
    E^V_\textnormal{reconst}
        =   \frac{1}{|H|\cdot \left( \max(V)-\min(V) \right)}
            \Int{\left| R(\vx) - V(\vx) \right|}{\vx,{\vx \in H}}
             \text{ ,}
\end{equation}
%
where $V$ is the original scalar field, $R$ is the reconstructed data, $H$ is
the convex hull of all points used to reconstruct the data, and $|H|$ is the
volume of H. The range of values of variable $V$ is described by
$\max(V)-\min(V)$.

%
\begin{figure}[t]
    \centering
    \setlength\figureheight{0.2\textheight}
    \setlength\figurewidth{0.87\textwidth}
    \input{figures/mean_abs_fit.tikz}
    \vspace*{-7mm}
    \caption{Mean \ac{RMS} fitting error for all variables. Note that not all data sets
    contain the same variables.}
    \label{fig:fitting_rms_error}
\end{figure}
%
% 2. Total errors (MSE, PSNR?) after reconstruction
% Compare different seeding densities and different interpolation schemes
% Naive downsampling of original data to same compression factor as reference
% Only consider data in convex hull of profiles
% Compared different parameters for k and p and selected best one in average for comparison
% naive downsampling better for low compression factors as to be expected
% Much lower error rates than naive downsampling for higher compression factors
% Linear better than knn except for few places
% Compression factors up to about 1:150 without terrible errors
%
\begin{figure}[p]
    \setlength\figureheight{0.26\textheight}
    \setlength\figurewidth{0.8\textwidth}
    % \input{figures/msevcr_stack_small.tikz}\\
    % \input{figures/msevcr_stack_data3_small.tikz}\\
    % \input{figures/msevcr_stack_data2.tikz}\\
    \input{figures/msevcr_stacks_small.tikz}
    \vspace*{-6mm}
    \caption{
    Error vs. reduction ratio for selected variables. The plots corresponding to
    each variable are shifted by a constant increment. The horizontal lines
    signify the zero-levels of the respective plots.}
    \label{fig:CRvMSE}
\end{figure}

% \begin{figure}[t]
%   \centering
%   \setlength\figureheight{5cm}
%   \setlength\figurewidth{0.7\textwidth}
%   \input{figures/msevcr_stack_data3_small.tikz}
%   \caption{Error vs. reduction ratio (\textsc{Syngas II}). See Figure
%   \ref{fig:CRvMSE} for explanation.
%   }
%   \label{fig:CRvMSE3}
% \end{figure}

% \begin{figure}[t]
%   \centering
%   \setlength\figureheight{5cm}
%   \setlength\figurewidth{0.7\textwidth}
%   \input{figures/msevcr_stack_data2.tikz}
%   \caption{Error vs. reduction ratio (\textsc{Syngas III}). See Figure
%   \ref{fig:CRvMSE} for explanation.
%   }
%   \label{fig:CRvMSE2}
% \end{figure}

\begin{figure}[tp]
    \centering
    \setlength\figurewidth\textwidth
    \input{figures/comparison_lin_knn_down_tikz.tex}
    \vspace*{-5mm}
    \caption{
        Top: Comparison of reconstruction results for \ce{H} of \textsc{Syngas
        II} with $q=\num{36}$ (ca. \num{2500} profile lines, $c=\num{321}$).
        Bottom: Reconstruction results for \ce{O2} of \textsc{Syngas I} using
        linear interpolation with different sample densities (detail view).}
    \label{fig:comp}
\end{figure}
%

%
We compared the results of linear and \ac{kANN} interpolation for
reconstruction.
%
Our experiments show that for the \ac{kANN} interpolation a combination of five
nearest neighbors weighted with a Shepard weighting function using an exponent
of \num{20} gave the lowest errors.
%
Therefore, we illustrate the results for these parameters only.
%
% Since the \ac{kANN} interpolation uses Shepard weights, we have two parameters: $k$
% for the number of approximate nearest neighbors to consider, and $p$, which is
% the exponent in the Shepard weighting function. We computed the error for
% values of $k$ ranging from $5$ to $20$, and of $p$ ranging from $1$ to $20$. A
% combination of $k=5$ and $p=20$ yielded the best results. Therefore, we
% illustrate the results for these parameters.
%
We also compared our results to the error introduced by naively downsampling
the data to the same storage size needed by the sparse representation.
%
This is the currently still the most common way of reducing the size of \ac{DNS}
data.
%
For comparison with a dedicated compression algorithm, we used the
well-established \ac{3D}-\ac{SPIHT} algorithm~\cite{Kim2000} implemented in the
QccPack library~\cite{Fowler2000} on our data.
%

% To be able to evaluate the ability of the sparse form to accurately represent
% the original data, we must also compare it to the method used previously to
% reduce the size of DNS data: na\"{i}ve downsampling. For comparison, we
% downsampled the original data to the same storage size as our sparse
% representation, then upsampled it back to the original resolution. We then
% used the same metric $E^V_\text{reconst}$ to compute the error introduced by
% this process.

%
\Cref{fig:CRvMSE} shows the reduction ratio $c$ vs. the reconstruction error
$E_\textnormal{reconst}^V$ for all tested methods for selected variables.
%
Please note that for \textsc{Syngas II}, higher reduction ratios are achieved
than for \textsc{Syngas I}, due to the flame in the former being relatively
smaller.
%
For small reduction ratios, the downsampling approach performs better, because
it does not introduce errors due to model assumptions.
%
For higher reduction ratios, which are needed in practice, our sparse
representation always performs significantly better.
%
It is also apparent that linear interpolation performs better than \ac{kANN} in
almost all cases.
%
As a dedicated compression algorithm, \ac{3D}-\ac{SPIHT} naturally achieves
better reconstruction quality than our approach.
%
It is however necessary to decompress the data back to its full size before any
analysis can be carried out, while our sparse representation can directly be
used for flamelet analysis and visualization of feature surfaces without prior
reconstruction.
%

\begin{figure}[t]
    \setlength\figureheight{0.2\textheight}
    \setlength\figurewidth{0.8\textwidth}
    \centering
    % \hbox{\hspace{-1.5em}
    \input{figures/compr_table.tikz}
    % }
    \vspace*{-7mm}
    \tikzset{external/export=false}
    \caption{
    Reduction ratio $c$ and computation time $t$ for different seeding densities
    q. Plot shows mean
    (\protect\tikz[baseline=-0.5ex]\protect\draw[thick, black] (0, 0) -- (4mm, 0);)
    and standard deviation
    (\protect\tikz[baseline=-0.5ex]\protect\draw[thick, black, dashed] (0, 0) -- (4mm, 0);)
    over eight time steps of data set \textsc{Hydrogen} and values for the synthetic
    data set
    (\protect\tikz[baseline=-0.5ex]\protect\draw[thick, black, dashdotted] (0, 0) -- (4mm, 0);)
    .}
    \label{fig:compression_table}
    \tikzset{external/export=true}
\end{figure}

%
To further illustrate the data reduction performance of our approach, we tested
it on eight time steps of the data set \textsc{Hydrogen}.
%
This data set is from a turbulent premixed spherical hydrogen flame and has a
resolution of $\num{400}^3$ voxels and \num{11} variables.
%
This is a typical size for our cooperation partner.
%
One time step amounts to about \SI{5}{\giga\byte} of data.
%
The whole simulation has tens of thousands of time steps.
%
Even storing a fraction of them quickly results in terabytes of data.
%
We selected eight time steps from a late (most complex) stage of the simulation.
%
Choosing $q=5$, which retains good accuracy for reconstruction, we reduced them
to about \SI{30}{\mega\byte} each.
%
This means only \SI{0.6}{\percent} of the original storage space is required.
%

%
For a scalability test, we generated a synthetic data set with $\num{900}^3$
voxels and \num{11} variables.
%
This is a typical size for a modern large-scale \ac{DNS} run.
%
We created noise based on an isotropic turbulence frequency
spectrum~\cite{Ferrante2003}.
%
This noise was added to a low frequency component to emulate larger flame
structures with smaller surface perturbations.
%
Thresholding produces a surface on which average profiles of the different
variables were superimposed.
%
Domain experts confirmed the similarity of the result to real simulations,
making it suitable for scalability tests.
%

%
Since each profile line has to be processed separately, the run time of the
algorithm is approximately linear in the number of seed points, and thus depends
indirectly on the flame surface area and structure.
%
\Cref{fig:compression_table} shows run times and compression ratios for
the synthetic data set and \textsc{Hydrogen}.
%
Data set size and flame surface area of the synthetic data set are one order
of magnitude higher than that of \textsc{Hydrogen}.
%
This results in a run time which is also one order of magnitude higher,
confirming the scalability of our approach.
%

% We have shown that full scalar fields can be reconstructed with good agreement
% to the original data from a sparse representation requiring a small fraction of
% the storage space. This representation explicitly captures features such
% as the points of steepest slope of reactants and products and the point
% of maximum concentration of intermediate species. In the following section,
% these feature points are used to visualize the relations between feature
% surfaces of different variables.
%
% our compression approach maintains good quality while
% providing high reduction ratios. It also preserves features of the data such
% as the points of steepest slope of reactants and products as well as the point
% of maximum concentration of intermediate species. These feature points can now
% be used to visualize the relations between feature surfaces of different
% variables.
%
% subsection evaluation_compression (end)
%
% section reconstruction (end)
